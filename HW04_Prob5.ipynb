{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb423c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from random import randint\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "is_MSE_loss = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1f3515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 1: Prepare dataset\n",
    "'''\n",
    "\n",
    "label_1, label_2 = 4, 9\n",
    "\n",
    "train_set = datasets.MNIST(root='./mnist_data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "idx = (train_set.targets==label_1) + (train_set.targets==label_2)\n",
    "train_set.data = train_set.data[idx]\n",
    "train_set.targets = train_set.targets[idx]\n",
    "train_set.targets[train_set.targets==label_1] = -1\n",
    "train_set.targets[train_set.targets==label_2] = 1\n",
    "\n",
    "test_set = datasets.MNIST(root='./mnist_data', train=False, transform=transforms.ToTensor())\n",
    "idx = (test_set.targets == label_1) + (test_set.targets == label_2)\n",
    "test_set.data = test_set.data[idx]\n",
    "test_set.targets = test_set.targets[idx]\n",
    "test_set.targets[test_set.targets==label_1] = -1\n",
    "test_set.targets[test_set.targets==label_2] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c22b608",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 2: Define the neural network class\n",
    "'''\n",
    "\n",
    "class LR(nn.Module):\n",
    "    def __init__(self, input_dim=28*28):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1, bias=True)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x.float().view(-1, 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3887ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 3: Create the model, specify loss function and optimizer\n",
    "'''\n",
    "model = LR()\n",
    "\n",
    "def logistic_loss(output, target):\n",
    "    if is_MSE_loss == False:\n",
    "        return -torch.nn.functional.logsigmoid(target*output)\n",
    "    else:\n",
    "        z, y = output, target\n",
    "        first_term = 1/2/len(output)*(1-y)*((1-torch.sigmoid(-z))**2 + (torch.sigmoid(z))**2)\n",
    "        second_term = 1/2/len(output)*(1+y)*((torch.sigmoid(-z))**2 + (1-torch.sigmoid(z))**2)\n",
    "        return first_term + second_term\n",
    "\n",
    "loss_function = logistic_loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d192f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time ellapsed in training is: 0.3452332019805908\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step 4: Train model with SGD\n",
    "'''\n",
    "import time\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    ind = randint(0, len(train_set.data))\n",
    "    image, label = train_set.data[ind], train_set.targets[ind]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    train_loss = loss_function(model(image), label.float())\n",
    "    train_loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time ellapsed in training is: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d90b77cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test set] Average loss: 0.9199, Accuracy: 1070/1991 53.74%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step 5: Test model (Evaluate the accuracy)\n",
    "'''\n",
    "test_loss, correct = 0, 0\n",
    "misclassified_ind = []\n",
    "correct_ind = []\n",
    "\n",
    "for ind in range(len(test_set.data)):\n",
    "    image, label = test_set.data[ind], test_set.targets[ind]\n",
    "    output = model(image)\n",
    "    test_loss += loss_function(output, label.float()).item()\n",
    "    \n",
    "    if output.item() * label.item() >= 0:\n",
    "        correct += 1\n",
    "        correct_ind += [ind]\n",
    "    else:\n",
    "        misclassified_ind += [ind]\n",
    "\n",
    "print('[Test set] Average loss: {:.4f}, Accuracy: {}/{} {:.2f}%\\n'.format(\n",
    "test_loss / len(test_set.data), correct, len(test_set.data), 100 * correct / len(test_set.data)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405d287a",
   "metadata": {},
   "source": [
    "### Performance comparison, compared to minimizing KL divergence\n",
    "The performance is much worse when minimizing the MSELoss, compared to  minimizing the KL divergence.\n",
    "Using MSELoss is a bad idea, especially because of task that the model needs to perform. The problem we're facing is a \"binary classification\" problem. Using the MSELoss assumes that $Y_i$ can take on continuous values, and that the $Y_i$'s follow a normal distribution centered around some mean value. This isn't the case for binary classification \\- the labels are discrete and can take on only two values, i.e. $Y_i$'s follow a Bernoulli distribution with success probability $p\\approx0.5$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
